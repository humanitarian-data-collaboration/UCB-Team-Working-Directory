{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hdx.utilities.easy_logging import setup_logging\n",
    "from hdx.hdx_configuration import Configuration\n",
    "from hdx.data.dataset import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Configuration.create(hdx_site='prod', user_agent='A_Quick_Example', hdx_read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.read_from_hdx('acled-data-for-south-africa')\n",
    "print(type(dataset))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hdx.facades.simple import facade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = dataset.get_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "organization = dataset.get_organization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(organization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#uploading a new dataset\n",
    "from fastText import load_model\n",
    "from os import path, getcwd\n",
    "import re, string\n",
    "import pickle\n",
    "\n",
    "'''\n",
    "This program reads an untagged dataset and tags it using a trained classifier.\n",
    "\n",
    "Input: Raw .xlsx file without tags from the HDX.\n",
    "NOTE: This PoC has been written for .xlsx files but could easily be rewritten to handle other formats\n",
    "\n",
    "Output: The same .xlsx but with an additional row containing the predicted hashtags\n",
    "'''\n",
    "\n",
    "\n",
    "def split_punctuation(value): # split strings on punctuation characters:\n",
    "    table = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "    return value.translate(table)\n",
    "\n",
    "\n",
    "def lower_case_cond(value): # lowercase only words which are all uppercase\n",
    "    word_list = value.split()\n",
    "    for i, word in enumerate(word_list):\n",
    "        if word.isupper():\n",
    "            word_list[i] = word.lower()\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "\n",
    "def split_uppercase(value): # split strings on uppercase\n",
    "    return re.sub(r'([A-Z])', r' \\1', str(value))\n",
    "\n",
    "\n",
    "def remove_excess_whitespace(value):\n",
    "    return ' '.join(value.split())\n",
    "\n",
    "\n",
    "def format_header(header):\n",
    "    header = str(header)\n",
    "    header = split_punctuation(header)\n",
    "    header = lower_case_cond(header)\n",
    "    header = split_uppercase(header)\n",
    "    header = remove_excess_whitespace(header)\n",
    "    header = header.lower()\n",
    "    return header\n",
    "\n",
    "\n",
    "input_file = \"ao-airports.xlsx\"\n",
    "pretrained_fasttext_model = 'wiki.en.bin'   # https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip\n",
    "\n",
    "d = path.dirname('__file__')\n",
    "df = pd.read_excel(path.join(d, \"Unlabeled Test Data\", input_file))    # Path to untagged dataset\n",
    "\n",
    "# Preprocessing\n",
    "headers = list(df)\n",
    "headers = [format_header(x) for x in headers]\n",
    "\n",
    "# Load word embedding model for feature generation\n",
    "fastText_model = load_model(pretrained_fasttext_model)\n",
    "print(\"Pre-trained model loaded successfully!\\n\")\n",
    "\n",
    "# Convert dataset headers into word embeddings\n",
    "headers = [fastText_model.get_sentence_vector(x).tolist() for x in headers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_uppercase(value):     # split strings on uppercase\n",
    "    return re.sub(r'([A-Z])', r' \\1', str(value))\n",
    "\n",
    "\n",
    "def lower_case_cond(value):     # lowercase only words which are all uppercase\n",
    "    word_list = value.split()\n",
    "    for i, word in enumerate(word_list):\n",
    "        if word.isupper():\n",
    "            word_list[i] = word.lower()\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "\n",
    "def split_punctuation(value):   # split strings on punctuation characters:\n",
    "    table = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "    return value.translate(table)\n",
    "\n",
    "\n",
    "def remove_excess_whitespace(value):\n",
    "    return ' '.join(value.split())\n",
    "\n",
    "input_file = \"ao-airports_tagged.xls\"\n",
    "#input_file = \"Unlabeled Test Data\" + \"/\" + input_file\n",
    "output_file = 'cleaned_hxl_data.csv'\n",
    "\n",
    "df2 = pd.read_excel(input_file)\n",
    "df2.head()\n",
    "label = df2.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2['Text header'] = df2.index.map(lambda x: str(x))\n",
    "df2['Text header'] = df2['Text header'].map(lambda x: split_punctuation(x))\n",
    "df2['Text header'] = df2['Text header'].map(lambda x: lower_case_cond(x))\n",
    "df2['Text header'] = df2['Text header'].map(lambda x: split_uppercase(x))\n",
    "df2['Text header'] = df2['Text header'].map(lambda x: remove_excess_whitespace(x))\n",
    "df2['Text header'] = df2['Text header'].map(lambda x: x.lower())\n",
    "\n",
    "header = df2[['Text header']]\n",
    "\n",
    "training_data = pd.concat([label, header], axis=1)\n",
    "training_data.to_csv(path.join(d, output_file), index=False, sep=',', encoding='utf-8', quotechar=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the cleaned HXL data\n",
    "\n",
    "input_file = output_file\n",
    "input_file = pd.read_csv(input_file , delimiter=',', encoding = 'utf-8')\n",
    "df[\"Text_header\"] = df[\"Text_header\"].map(lambda x: re.sub(' +', ' ', str(x)))\n",
    "\n",
    "# Get a vector representation of each header\n",
    "df['Word_embedding'] = df['Text_header'].map(lambda x: fastText_model.get_sentence_vector(str(x)))\n",
    "print(\"Word embeddings extracted!\\n\")\n",
    "\n",
    "# Save the vectorized data\n",
    "df.to_csv(output_file, sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "'''\n",
    "This program trains an MLP classifier to predict HXL hashtags.\n",
    "If you are training a classifier on a new dataset, it is adviced to first tune the parameters of the model.\n",
    "\n",
    "Input: .csv file containing hashtags, header strings and their corresponding word embeddings from extract_features.py\n",
    "\n",
    "Output: A pickled MLP classifier. Also the model is tested on a test set, the classification accuracy is printed \n",
    "along with the confusion matrix.\n",
    "'''\n",
    "\n",
    "\n",
    "def format_embeddings(embedding):\n",
    "    \"\"\"Fix some formatting issues from feature extraction\"\"\"\n",
    "    embedding = embedding.replace('\\r\\n', '')\n",
    "    embedding = embedding.replace('[', '')\n",
    "    embedding = embedding.replace(']', '')\n",
    "    return np.fromstring(embedding, dtype=float, sep=' ').tolist()\n",
    "\n",
    "\n",
    "input_file = 'wordembedding_data.csv'\n",
    "output_file = 'MLPclassifier.pkl'\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(input_file, delimiter=',', encoding='utf-8')\n",
    "\n",
    "df['Class'] = df['Hashtag']\n",
    "df['Word_embedding'] = df['Word_embedding'].map(lambda x: format_embeddings(x))\n",
    "\n",
    "# Remove infrequent classes\n",
    "threshold = 5   # include only rows with at least this many points\n",
    "class_count = df['Class'].value_counts()\n",
    "removal = class_count[class_count <= threshold].index\n",
    "df['Class'] = df['Class'].replace(removal, np.nan)\n",
    "df = df.dropna()\n",
    "\n",
    "df = df[['Class', 'Word_embedding']].copy()\n",
    "df_labels = df.Class.unique()\n",
    "df_labels = np.sort(df_labels, axis=-1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Word_embedding'], df['Class'], test_size=0.33, random_state=0)\n",
    "\n",
    "\n",
    "# Train the classifier with the parameters as specified\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "clf.fit(X_train.values.tolist(), y_train.values.tolist())\n",
    "test_score = clf.score(X_test.tolist(), y_test.tolist())\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = clf.predict(X_test.values.tolist())\n",
    "confmatrix = confusion_matrix(y_test.values.tolist(), y_pred, df_labels)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(confmatrix, annot=True, fmt='d', xticklabels=df_labels, yticklabels=df_labels, vmax=80)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Save the trained classifier for later use\n",
    "pickle.dump(clf, open(output_file, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained classifier\n",
    "clf = pickle.load(open(\"MLPclassifier.pkl\", 'rb'))\n",
    "\n",
    "# Predict tags\n",
    "tags = clf.predict(headers)\n",
    "\n",
    "# Insert row of tags into the dataset\n",
    "df.loc[-1] = tags\n",
    "df.index = df.index + 1  # shifting index\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "writer = pd.ExcelWriter(path.join(d,\"..\",\"Unlabeled Test Data\",\"Tagged-\"+input_file), engine='xlsxwriter')\n",
    "df.to_excel(writer, index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
