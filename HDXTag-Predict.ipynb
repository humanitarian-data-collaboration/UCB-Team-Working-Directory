{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdx.utilities.easy_logging import setup_logging\n",
    "from hdx.hdx_configuration import Configuration\n",
    "from hdx.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Configuration.create(hdx_site='prod', user_agent='A_Quick_Example', hdx_read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.read_from_hdx('acled-conflict-data-for-africa-1997-lastyear')\n",
    "print(dataset.get_dataset_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = Dataset.search_in_hdx('ACLED', rows=10)\n",
    "print(datasets)\n",
    "resources = Dataset.get_all_resources(datasets)\n",
    "print(resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url, path = resources[0].download()\n",
    "print('Resource URL %s downloaded to %s' % (url, path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "import re, string\n",
    "\n",
    "\n",
    "'''\n",
    "This program pre-processes the data consisting of the headers of HDX datasets.\n",
    "The idea is to prepare the strings for the word embedding model which works best if words are separated by \n",
    "blank space. Thus the main pre-processing steps are to split the strings on punctuation characters, split on\n",
    "single capital letters, lowercase everything and remove excess whitespace.\n",
    "Input: .xlsx file containing at least the columns 'Hashtag' and 'Text header'. It is recommended that the input file\n",
    "be deduplicated so as not to include repetitions of identical file structures.\n",
    "Output: .csv file where each row contains a hashtag and a cleaned header string\n",
    "'''\n",
    "\n",
    "\n",
    "def split_uppercase(value):     # split strings on uppercase\n",
    "    return re.sub(r'([A-Z])', r' \\1', str(value))\n",
    "\n",
    "\n",
    "def lower_case_cond(value):     # lowercase only words which are all uppercase\n",
    "    word_list = value.split()\n",
    "    for i, word in enumerate(word_list):\n",
    "        if word.isupper():\n",
    "            word_list[i] = word.lower()\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "\n",
    "def split_punctuation(value):   # split strings on punctuation characters:\n",
    "    table = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "    return value.translate(table)\n",
    "\n",
    "\n",
    "def remove_excess_whitespace(value):\n",
    "    return ' '.join(value.split())\n",
    "\n",
    "\n",
    "input_file = 'hxl-hashtags-and-headers-DEDUPLICATED-20180807.xlsx'\n",
    "output_file = 'cleaned_hxl_data.csv'\n",
    "\n",
    "\n",
    "### d = path.dirname(__file__) ###\n",
    "\n",
    "df = pd.read_excel(input_file)\n",
    "label = df[['Hashtag']]\n",
    "\n",
    "df['Text header'] = df['Text header'].map(lambda x: str(x))\n",
    "df['Text header'] = df['Text header'].map(lambda x: split_punctuation(x))\n",
    "df['Text header'] = df['Text header'].map(lambda x: lower_case_cond(x))\n",
    "df['Text header'] = df['Text header'].map(lambda x: split_uppercase(x))\n",
    "df['Text header'] = df['Text header'].map(lambda x: remove_excess_whitespace(x))\n",
    "df['Text header'] = df['Text header'].map(lambda x: x.lower())\n",
    "\n",
    "header = df[['Text header']]\n",
    "\n",
    "training_data = pd.concat([label, header], axis=1)\n",
    "training_data.to_csv(path.join(d, output_file), index=False, sep=',', encoding='utf-8', quotechar=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastText import load_model\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "'''\n",
    "This program extracts features from the cleaned HXL headers by converting them to word embeddings.\n",
    "The word embeddings used here are 300-dim fastText embeddings. They are loaded from a large (~10GB) fastText model\n",
    "which can be downloaded here: https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip\n",
    "Note that the .zip contains a .bin file and a .vec file. These are different formats for storing the fastText model,\n",
    "use the .bin file whenever possible.\n",
    "Input: .csv file where each row contains a hashtag and a cleaned header string\n",
    "Output: .csv file containing hashtags, header strings and their corresponding word embeddings\n",
    "NOTE: this output formatting is not ideal and currently has to be handled ad hoc in the program which trains the ML \n",
    "model. It should be changed to something more suitable for storing large vectors, e.g. .xml, .pickle, etc.\n",
    "'''\n",
    "\n",
    "\n",
    "input_file = 'cleaned_hxl_data.csv'\n",
    "output_file = 'wordembedding_data.csv'\n",
    "pretrained_fasttext_model = 'wiki.en.bin'   # https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip\n",
    "\n",
    "\n",
    "# Load the fastText model\n",
    "fastText_model = load_model(pretrained_fasttext_model)\n",
    "print(\"Pre-trained model loaded successfully!\\n\")\n",
    "\n",
    "# Read the cleaned HXL data\n",
    "df = pd.read_csv(input_file , delimiter=',', encoding='utf-8')\n",
    "df[\"Text_header\"] = df[\"Text_header\"].map(lambda x: re.sub(' +', ' ', str(x)))\n",
    "\n",
    "# Get a vector representation of each header\n",
    "df['Word_embedding'] = df['Text_header'].map(lambda x: fastText_model.get_sentence_vector(str(x)))\n",
    "print(\"Word embeddings extracted!\\n\")\n",
    "\n",
    "# Save the vectorized data\n",
    "df.to_csv(output_file, sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "'''\n",
    "This program tunes the parameters of the Multilayer Perceptron model through a crossvalidated gridsearch.\n",
    "Input: .csv file containing hashtags, header strings and their corresponding word embeddings from extract_features.py\n",
    "Output: A pickled MLP classifier. Also the optimal parameter values are printed.\n",
    "'''\n",
    "\n",
    "\n",
    "def format_embeddings(embedding):\n",
    "    \"\"\"Fix some formatting issues from feature extraction\"\"\"\n",
    "    embedding = embedding.replace('\\r\\n', '')\n",
    "    embedding = embedding.replace('[', '')\n",
    "    embedding = embedding.replace(']', '')\n",
    "    return np.fromstring(embedding, dtype=float, sep=' ').tolist()\n",
    "\n",
    "\n",
    "input_file = 'wordembedding_data.csv'\n",
    "output_file = 'MLPclassifier.pkl'\n",
    "\n",
    "\n",
    "# Read and process data\n",
    "df = pd.read_csv(input_file, delimiter=',', encoding='utf-8')\n",
    "\n",
    "df['Class'] = df['Hashtag']\n",
    "df['Word_embedding'] = df['Word_embedding'].map(lambda x: format_embeddings(x))\n",
    "\n",
    "threshold = 5   # include only rows with at least this many points\n",
    "class_count = df['Class'].value_counts()\n",
    "removal = class_count[class_count <= threshold].index\n",
    "df['Class'] = df['Class'].replace(removal, np.nan)\n",
    "df = df.dropna()\n",
    "\n",
    "df = df[['Class', 'Word_embedding']].copy()\n",
    "\n",
    "X = df['Word_embedding'].values.tolist()\n",
    "y = df['Class'].values.tolist()\n",
    "\n",
    "\n",
    "# Parameter grid to search through\n",
    "param_grid = [\n",
    "    {\n",
    "        'solver' : ['adam', 'lbfgs'],\n",
    "        'alpha' : [0.001, 0.01, 0.1],\n",
    "        'hidden_layer_sizes' : [50, 75, 100, 150, 200],\n",
    "        'activation' : ['tanh', 'relu']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Tune parameters\n",
    "clf = GridSearchCV(MLPClassifier(), param_grid, cv=3, scoring='accuracy', verbose=10)\n",
    "clf.fit(X,y)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)\n",
    "\n",
    "# Save trained classifier\n",
    "pickle.dump(clf, open(output_file, 'wb'))\n",
    "\n",
    "\n",
    "# 2018-08-20: Good parameter choices found to be:\n",
    "# 'activation': 'relu', 'alpha': 0.001, 'epsilon': 1e-08, 'hidden_layer_sizes': 150, 'solver': 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "'''\n",
    "This program trains an MLP classifier to predict HXL hashtags.\n",
    "If you are training a classifier on a new dataset, it is adviced to first tune the parameters of the model.\n",
    "Input: .csv file containing hashtags, header strings and their corresponding word embeddings from extract_features.py\n",
    "Output: A pickled MLP classifier. Also the model is tested on a test set, the classification accuracy is printed \n",
    "along with the confusion matrix.\n",
    "'''\n",
    "\n",
    "\n",
    "def format_embeddings(embedding):\n",
    "    \"\"\"Fix some formatting issues from feature extraction\"\"\"\n",
    "    embedding = embedding.replace('\\r\\n', '')\n",
    "    embedding = embedding.replace('[', '')\n",
    "    embedding = embedding.replace(']', '')\n",
    "    return np.fromstring(embedding, dtype=float, sep=' ').tolist()\n",
    "\n",
    "\n",
    "input_file = 'wordembedding_data.csv'\n",
    "output_file = 'MLPclassifier.pkl'\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(input_file, delimiter=',', encoding='utf-8')\n",
    "\n",
    "df['Class'] = df['Hashtag']\n",
    "df['Word_embedding'] = df['Word_embedding'].map(lambda x: format_embeddings(x))\n",
    "\n",
    "# Remove infrequent classes\n",
    "threshold = 5   # include only rows with at least this many points\n",
    "class_count = df['Class'].value_counts()\n",
    "removal = class_count[class_count <= threshold].index\n",
    "df['Class'] = df['Class'].replace(removal, np.nan)\n",
    "df = df.dropna()\n",
    "\n",
    "df = df[['Class', 'Word_embedding']].copy()\n",
    "df_labels = df.Class.unique()\n",
    "df_labels = np.sort(df_labels, axis=-1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Word_embedding'], df['Class'], test_size=0.33, random_state=0)\n",
    "\n",
    "\n",
    "# Train the classifier with the parameters as specified\n",
    "clf = MLPClassifier(activation='relu', alpha=0.001, epsilon=1e-08, hidden_layer_sizes=150, solver='adam')\n",
    "clf.fit(X_train.values.tolist(), y_train.values.tolist())\n",
    "test_score = clf.score(X_test.tolist(), y_test.tolist())\n",
    "print(\"Classification accuracy on test set: %s\" %test_score)\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = clf.predict(X_test.values.tolist())\n",
    "confmatrix = confusion_matrix(y_test.values.tolist(), y_pred, df_labels)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(confmatrix, annot=True, fmt='d', xticklabels=df_labels, yticklabels=df_labels, vmax=80)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Save the trained classifier for later use\n",
    "pickle.dump(clf, open(output_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wordcloud\n",
    "from os import path\n",
    "\n",
    "'''\n",
    "This program generates wordclouds of the table headers associated with different HXL hashtags.\n",
    "Input: .csv file containing at least the columns 'Hashtag' and 'Text header'\n",
    "Output: A set of .png figures of the word clouds for the hashtags\n",
    "'''\n",
    "\n",
    "\n",
    "input_file = \"hdx-hashtags-list.csv\"    # This is the raw HXL csv I got from David Megginson\n",
    "\n",
    "\n",
    "d = path.dirname(__file__)\n",
    "\n",
    "# Read and process data\n",
    "df = pd.read_csv(input_file)\n",
    "df.columns = df.columns.str.lower()\n",
    "cols = df.columns\n",
    "cols = cols.map(lambda x: x.replace(' ', '_') if isinstance(x, (str, bytes)) else x)\n",
    "df.columns = cols\n",
    "df[\"text_header\"] = df[\"text_header\"].str.lower()\n",
    "df[\"text_header\"] = df[\"text_header\"].replace('_', ' ', regex=True)\n",
    "\n",
    "tagList = df.hashtag.unique()   # List all unique hashtags in the dataset\n",
    "output = pd.DataFrame(columns=['Hashtag','Count','Unique headers','Score'])\n",
    "\n",
    "i=0\n",
    "for tag in tagList:\n",
    "    # Compute various statistics\n",
    "    df_tag = df.loc[df['hashtag'] == tag]\n",
    "    count = df_tag.shape[0]\n",
    "    unique = len(df_tag['text_header'].unique())\n",
    "    output.loc[i] = [tag, count, unique, unique/count]\n",
    "    i+=1\n",
    "\n",
    "output_top100 = output.loc[output[\"Count\"]>100]  # Create word clouds only for the tags with >100 occurrences\n",
    "\n",
    "for index, row in output_top100.iterrows():\n",
    "    # Create wordclouds\n",
    "    hashtag = row['Hashtag']\n",
    "    df_wc = df.loc[df['hashtag'] == hashtag]\n",
    "    tuples = tuple([tuple(x) for x in df_wc.text_header.value_counts().reset_index().values])\n",
    "    tuples = dict(tuples)\n",
    "    cloud = wordcloud.WordCloud(background_color=\"white\", max_font_size=40)\n",
    "    cloud.generate_from_frequencies(tuples)\n",
    "    plt.figure()\n",
    "    plt.imshow(cloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(70,230,\"Hashtag Occurrence: %s, Unique Headers: %s\" %(row['Count'], row['Unique headers']))\n",
    "    plt.title(hashtag, fontsize=18)\n",
    "    plt.savefig(path.join(d, \"wordcloud\", \"wordcloud%s.png\" %hashtag))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'TAG NEW DATASET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastText import load_model\n",
    "from os import path\n",
    "import re, string\n",
    "import pickle\n",
    "\n",
    "'''\n",
    "This program reads an untagged dataset and tags it using a trained classifier.\n",
    "Input: Raw .xlsx file without tags from the HDX.\n",
    "NOTE: This PoC has been written for .xlsx files but could easily be rewritten to handle other formats\n",
    "Output: The same .xlsx but with an additional row containing the predicted hashtags\n",
    "'''\n",
    "\n",
    "\n",
    "def split_punctuation(value): # split strings on punctuation characters:\n",
    "    table = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "    return value.translate(table)\n",
    "\n",
    "\n",
    "def lower_case_cond(value): # lowercase only words which are all uppercase\n",
    "    word_list = value.split()\n",
    "    for i, word in enumerate(word_list):\n",
    "        if word.isupper():\n",
    "            word_list[i] = word.lower()\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "\n",
    "def split_uppercase(value): # split strings on uppercase\n",
    "    return re.sub(r'([A-Z])', r' \\1', str(value))\n",
    "\n",
    "\n",
    "def remove_excess_whitespace(value):\n",
    "    return ' '.join(value.split())\n",
    "\n",
    "\n",
    "def format_header(header):\n",
    "    header = str(header)\n",
    "    header = split_punctuation(header)\n",
    "    header = lower_case_cond(header)\n",
    "    header = split_uppercase(header)\n",
    "    header = remove_excess_whitespace(header)\n",
    "    header = header.lower()\n",
    "    return header\n",
    "\n",
    "\n",
    "input_file = \"data.xlsx\"\n",
    "pretrained_fasttext_model = 'wiki.en.bin'   # https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip\n",
    "\n",
    "d = path.dirname(__file__)\n",
    "df = pd.read_excel(path.join(d, \"..\", \"Unlabeled Test Data\", input_file))    # Path to untagged dataset\n",
    "\n",
    "# Preprocessing\n",
    "headers = list(df)\n",
    "headers = [format_header(x) for x in headers]\n",
    "\n",
    "# Load word embedding model for feature generation\n",
    "fastText_model = load_model(pretrained_fasttext_model)\n",
    "print(\"Pre-trained model loaded successfully!\\n\")\n",
    "\n",
    "# Convert dataset headers into word embeddings\n",
    "headers = [fastText_model.get_sentence_vector(x).tolist() for x in headers]\n",
    "\n",
    "# Load the pre-trained classifier\n",
    "clf = pickle.load(open(\"MLPclassifier.pkl\", 'rb'))\n",
    "\n",
    "# Predict tags\n",
    "tags = clf.predict(headers)\n",
    "\n",
    "# Insert row of tags into the dataset\n",
    "df.loc[-1] = tags\n",
    "df.index = df.index + 1  # shifting index\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "writer = pd.ExcelWriter(path.join(d,\"..\",\"Unlabeled Test Data\",\"Tagged-\"+input_file), engine='xlsxwriter')\n",
    "df.to_excel(writer, index=False)\n",
    "writer.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
